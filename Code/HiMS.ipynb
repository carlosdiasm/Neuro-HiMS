{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCVsRiH95gDz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "import warnings\n",
        "import numba\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MVq9bJs5h4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91c8f1e-2182-4641-e9a0-aa2df1e85faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montando o Google Drive...\n",
            "Mounted at /content/drive\n",
            "Drive montado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "print(\"Montando o Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Drive montado com sucesso!\")\n",
        "BASE_PATH = \"/content/drive/MyDrive/TCC\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYiJm3-w5zSC"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FMRISkimmedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset para imagens fMRI extraídas com algoritmo de skimming\n",
        "    Estrutura: slices/sub-x/orientacao/slice_orientacao_XX_idxYYY.png\n",
        "    Exemplo: slices/sub-10159/axial/slice_axial_01_idx022.png\n",
        "    \"\"\"\n",
        "    def __init__(self, metadata_df, base_path, transform=None, slices_per_view=2, validate=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_df: DataFrame com metadados dos pacientes\n",
        "            base_path: Caminho base dos dados (já aponta para a pasta slices)\n",
        "            transform: Transformações a aplicar nas imagens\n",
        "            slices_per_view: Número de slices por orientação\n",
        "            validate: Se True, valida disponibilidade de slices (padrão: True)\n",
        "        \"\"\"\n",
        "        self.metadata_df = metadata_df.reset_index(drop=True)\n",
        "        self.base_path = base_path\n",
        "        self.transform = transform\n",
        "        self.slices_per_view = slices_per_view\n",
        "        self.label_map = {'CONTROL': 0, 'SCHZ': 1}\n",
        "\n",
        "        # Valida que todos os pacientes têm slices disponíveis\n",
        "        if validate:\n",
        "            self._validate_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata_df)\n",
        "\n",
        "    def _validate_dataset(self):\n",
        "        \"\"\"\n",
        "        Valida que todos os pacientes no dataset têm slices disponíveis\n",
        "        Remove pacientes sem imagens processadas\n",
        "        \"\"\"\n",
        "        valid_indices = []\n",
        "\n",
        "        for idx, row in self.metadata_df.iterrows():\n",
        "            participant_id = row['participant_id']\n",
        "            subject_dir = os.path.join(self.base_path, f\"sub-{participant_id}\")\n",
        "\n",
        "            # Verifica se o diretório do sujeito existe\n",
        "            if not os.path.exists(subject_dir):\n",
        "                continue\n",
        "\n",
        "            # Verifica se tem subpastas de orientação e arquivos dentro\n",
        "            has_all_orientations = True\n",
        "            for orientation in ['sagital', 'coronal', 'axial']:\n",
        "                orientation_dir = os.path.join(subject_dir, orientation)\n",
        "\n",
        "                # Verifica se a pasta da orientação existe\n",
        "                if not os.path.exists(orientation_dir):\n",
        "                    has_all_orientations = False\n",
        "                    break\n",
        "\n",
        "                # Verifica se tem arquivos .png dentro\n",
        "                pattern = os.path.join(orientation_dir, f\"slice_{orientation}_*_idx*.png\")\n",
        "                files = glob.glob(pattern)\n",
        "                if len(files) == 0:\n",
        "                    has_all_orientations = False\n",
        "                    break\n",
        "\n",
        "            if has_all_orientations:\n",
        "                valid_indices.append(idx)\n",
        "\n",
        "        # Filtra o dataframe para manter apenas pacientes válidos\n",
        "        original_size = len(self.metadata_df)\n",
        "        self.metadata_df = self.metadata_df.iloc[valid_indices].reset_index(drop=True)\n",
        "        removed = original_size - len(self.metadata_df)\n",
        "\n",
        "        if removed > 0:\n",
        "            print(f\"{removed} pacientes removidos (sem slices processados)\")\n",
        "        print(f\"✓ {len(self.metadata_df)} pacientes válidos no dataset\")\n",
        "\n",
        "    def _get_ranked_slices(self, participant_id, orientation, n_slices):\n",
        "        \"\"\"\n",
        "        Carrega os N primeiros slices de uma orientação\n",
        "        Estrutura: slices/sub-x/orientacao/slice_orientacao_XX_idxYYY.png\n",
        "        Exemplo: slices/sub-10159/axial/slice_axial_01_idx022.png\n",
        "        \"\"\"\n",
        "        # Caminho da pasta da orientação específica\n",
        "        orientation_dir = os.path.join(self.base_path, f\"sub-{participant_id}\", orientation)\n",
        "\n",
        "        # Busca todos os arquivos da orientação\n",
        "        pattern = os.path.join(orientation_dir, f\"slice_{orientation}_*_idx*.png\")\n",
        "        all_files = glob.glob(pattern)\n",
        "\n",
        "        if len(all_files) == 0:\n",
        "            return []\n",
        "\n",
        "        # Extrai o número de ordem de cada arquivo (o XX em slice_orientacao_XX_idx)\n",
        "        rank_files = []\n",
        "        for filepath in all_files:\n",
        "            filename = os.path.basename(filepath)\n",
        "            # Padrão: slice_orientacao_XX_idxYYY.png\n",
        "            match = re.search(rf'slice_{orientation}_(\\d+)_idx', filename)\n",
        "            if match:\n",
        "                rank = int(match.group(1))\n",
        "                rank_files.append((rank, filepath))\n",
        "\n",
        "        # Ordena pelo número de ordem\n",
        "        rank_files.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Pega os N primeiros\n",
        "        selected_files = [filepath for rank, filepath in rank_files[:n_slices]]\n",
        "\n",
        "        return selected_files\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata_df.iloc[idx]\n",
        "        participant_id = row['participant_id']\n",
        "        label = self.label_map[row['diagnosis']]\n",
        "\n",
        "        # Carrega slices de cada orientação\n",
        "        orientations = ['sagital', 'coronal', 'axial']\n",
        "        all_slices = []\n",
        "        found_slices_count = 0\n",
        "\n",
        "        for orientation in orientations:\n",
        "            slice_paths = self._get_ranked_slices(participant_id, orientation, self.slices_per_view)\n",
        "\n",
        "            if len(slice_paths) < self.slices_per_view:\n",
        "                 print(f\"Participante {participant_id}, Orientação {orientation}: Encontrados {len(slice_paths)} slices, esperado {self.slices_per_view}\")\n",
        "\n",
        "            for img_path in slice_paths:\n",
        "                try:\n",
        "                    img = Image.open(img_path).convert('L')  # Grayscale\n",
        "                    if self.transform:\n",
        "                        img = self.transform(img)\n",
        "                    all_slices.append(img)\n",
        "                    found_slices_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao carregar slice {img_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Verifica o número total de slices encontrados\n",
        "        expected_total_slices = self.slices_per_view * len(orientations)\n",
        "        if found_slices_count != expected_total_slices:\n",
        "             print(f\"Participante {participant_id}: Total de slices {found_slices_count}, esperado {expected_total_slices}\")\n",
        "\n",
        "        if not all_slices:\n",
        "            print(f\"Participante {participant_id}: NENHUM slice carregado!\")\n",
        "            return None, None\n",
        "\n",
        "        # Verifica se todos os tensores têm o mesmo tamanho antes de empilhar\n",
        "        if len(all_slices) > 0:\n",
        "            first_shape = all_slices[0].shape\n",
        "            if not all(t.shape == first_shape for t in all_slices):\n",
        "                 print(f\"Participante {participant_id}: Slices com shapes diferentes! {[t.shape for t in all_slices]}\")\n",
        "                 return None, None\n",
        "\n",
        "        try:\n",
        "            images = torch.stack(all_slices, dim=0)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao empilhar slices para {participant_id}: {e}\")\n",
        "            print(f\"Shapes dos slices: {[t.shape for t in all_slices]}\")\n",
        "            return None, None\n",
        "\n",
        "        return images, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS-LV1XP5_MP"
      },
      "outputs": [],
      "source": [
        "class MultiSliceViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer que processa múltiplos slices individualmente e depois agrega\n",
        "    \"\"\"\n",
        "    def __init__(self, num_slices=6, num_classes=2, vit_model='vit_b_16', pretrained=True):\n",
        "        super(MultiSliceViT, self).__init__()\n",
        "\n",
        "        self.num_slices = num_slices\n",
        "\n",
        "        # Escolhe o modelo ViT\n",
        "        if vit_model == 'vit_b_16':\n",
        "            weights = 'DEFAULT' if pretrained else None\n",
        "            vit = models.vit_b_16(weights=weights)\n",
        "            feature_dim = 768\n",
        "        elif vit_model == 'vit_b_32':\n",
        "            weights = 'DEFAULT' if pretrained else None\n",
        "            vit = models.vit_b_32(weights=weights)\n",
        "            feature_dim = 768\n",
        "        elif vit_model == 'vit_l_16':\n",
        "            weights = 'DEFAULT' if pretrained else None\n",
        "            vit = models.vit_l_16(weights=weights)\n",
        "            feature_dim = 1024\n",
        "        else:\n",
        "            raise ValueError(f\"Modelo {vit_model} não suportado\")\n",
        "\n",
        "        # Adapta patch embedding para aceitar 1 canal (grayscale)\n",
        "        original_conv = vit.conv_proj\n",
        "        vit.conv_proj = nn.Conv2d(\n",
        "            1,\n",
        "            original_conv.out_channels,\n",
        "            kernel_size=original_conv.kernel_size,\n",
        "            stride=original_conv.stride,\n",
        "            padding=original_conv.padding,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        # Se pretrained, inicializa o novo conv somando os pesos RGB\n",
        "        if pretrained:\n",
        "            with torch.no_grad():\n",
        "                vit.conv_proj.weight = nn.Parameter(\n",
        "                    original_conv.weight.sum(dim=1, keepdim=True)\n",
        "                )\n",
        "\n",
        "        # Remove a cabeça de classificação original\n",
        "        vit.heads = nn.Identity()\n",
        "\n",
        "        self.encoder = vit\n",
        "\n",
        "        # Camadas de agregação\n",
        "        self.aggregation = nn.Sequential(\n",
        "            nn.Linear(feature_dim * num_slices, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor de shape (batch_size, num_slices, C, H, W)\n",
        "\n",
        "        Returns:\n",
        "            output: Tensor de shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size, num_slices, C, H, W = x.size()\n",
        "\n",
        "        slice_features = []\n",
        "        for i in range(num_slices):\n",
        "            slice_i = x[:, i, :, :, :]  # (batch_size, C, H, W)\n",
        "            feat = self.encoder(slice_i)  # (batch_size, feature_dim)\n",
        "            slice_features.append(feat)\n",
        "\n",
        "        # Concatena features de todos os slices\n",
        "        combined = torch.cat(slice_features, dim=1)  # (batch_size, feature_dim * num_slices)\n",
        "\n",
        "        # Classificação final\n",
        "        output = self.aggregation(combined)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t81axoquCFFx"
      },
      "outputs": [],
      "source": [
        "class MultiSliceViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer que processa múltiplos slices individualmente e depois agrega\n",
        "    com Transformer para capturar relações espaciais entre slices.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_slices=6,\n",
        "        num_classes=2,\n",
        "        vit_model='vit_b_16',\n",
        "        pretrained=True,\n",
        "        use_slice_transformer=True,\n",
        "        transformer_layers=2,\n",
        "        transformer_heads=8\n",
        "    ):\n",
        "        super(MultiSliceViT, self).__init__()\n",
        "\n",
        "        self.num_slices = num_slices\n",
        "        self.use_slice_transformer = use_slice_transformer\n",
        "\n",
        "        # Escolhe o modelo ViT\n",
        "        if vit_model == 'vit_b_16':\n",
        "            weights = 'DEFAULT' if pretrained else None\n",
        "            vit = models.vit_b_16(weights=weights)\n",
        "            self.feature_dim = 768\n",
        "        elif vit_model == 'vit_b_32':\n",
        "            weights = 'DEFAULT' if pretrained else None\n",
        "            vit = models.vit_b_32(weights=weights)\n",
        "            self.feature_dim = 768\n",
        "        elif vit_model == 'vit_l_16':\n",
        "            weights = 'DEFAULT' if pretrained else None\n",
        "            vit = models.vit_l_16(weights=weights)\n",
        "            self.feature_dim = 1024\n",
        "        else:\n",
        "            raise ValueError(f\"Modelo {vit_model} não suportado\")\n",
        "\n",
        "        # Adapta patch embedding para aceitar 1 canal (grayscale)\n",
        "        original_conv = vit.conv_proj\n",
        "        vit.conv_proj = nn.Conv2d(\n",
        "            1,\n",
        "            original_conv.out_channels,\n",
        "            kernel_size=original_conv.kernel_size,\n",
        "            stride=original_conv.stride,\n",
        "            padding=original_conv.padding,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        # Se pretrained, inicializa o novo conv somando os pesos RGB\n",
        "        if pretrained:\n",
        "            with torch.no_grad():\n",
        "                vit.conv_proj.weight = nn.Parameter(\n",
        "                    original_conv.weight.sum(dim=1, keepdim=True)\n",
        "                )\n",
        "\n",
        "        # Remove a cabeça de classificação original\n",
        "        vit.heads = nn.Identity()\n",
        "\n",
        "        self.encoder = vit\n",
        "\n",
        "        # Positional encoding para os slices (adiciona informação de ordem/posição)\n",
        "        self.slice_pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_slices, self.feature_dim) * 0.02\n",
        "        )\n",
        "\n",
        "        if use_slice_transformer:\n",
        "            # Transformer para capturar relações entre slices\n",
        "            encoder_layer = nn.TransformerEncoderLayer(\n",
        "                d_model=self.feature_dim,\n",
        "                nhead=transformer_heads,\n",
        "                dim_feedforward=self.feature_dim * 4,\n",
        "                dropout=0.1,\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.slice_transformer = nn.TransformerEncoder(\n",
        "                encoder_layer,\n",
        "                num_layers=transformer_layers\n",
        "            )\n",
        "\n",
        "            # Cabeça de classificação mais leve\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.LayerNorm(self.feature_dim),\n",
        "                nn.Linear(self.feature_dim, 256),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(256, num_classes)\n",
        "            )\n",
        "        else:\n",
        "            # Agregação simples (concatenação + MLP)\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(self.feature_dim * num_slices, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(512, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(128, num_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor de shape (batch_size, num_slices, C, H, W)\n",
        "\n",
        "        Returns:\n",
        "            output: Tensor de shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size, num_slices, C, H, W = x.size()\n",
        "\n",
        "        # Processa todos os slices em paralelo (muito mais eficiente!)\n",
        "        x_flat = x.view(batch_size * num_slices, C, H, W)\n",
        "        features = self.encoder(x_flat)  # (batch_size * num_slices, feature_dim)\n",
        "\n",
        "        # Reorganiza para separar batch e slices\n",
        "        features = features.view(batch_size, num_slices, self.feature_dim)\n",
        "\n",
        "        if self.use_slice_transformer:\n",
        "            # Adiciona positional encoding\n",
        "            features = features + self.slice_pos_embedding\n",
        "\n",
        "            # Captura relações entre slices com Transformer\n",
        "            features = self.slice_transformer(features)  # (batch_size, num_slices, feature_dim)\n",
        "\n",
        "            # Global average pooling sobre os slices\n",
        "            pooled = features.mean(dim=1)  # (batch_size, feature_dim)\n",
        "\n",
        "            # Classificação\n",
        "            output = self.classifier(pooled)\n",
        "        else:\n",
        "            # Concatena features de todos os slices\n",
        "            combined = features.view(batch_size, -1)  # (batch_size, feature_dim * num_slices)\n",
        "            output = self.classifier(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Exemplo de uso alternativo: Attention-based aggregation\n",
        "class MultiSliceViTWithAttention(MultiSliceViT):\n",
        "    \"\"\"\n",
        "    Variante que usa attention pooling ao invés de average pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        if self.use_slice_transformer:\n",
        "            # Attention pooling\n",
        "            self.attention_pool = nn.Sequential(\n",
        "                nn.Linear(self.feature_dim, 128),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(128, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_slices, C, H, W = x.size()\n",
        "\n",
        "        # Processa todos os slices em paralelo\n",
        "        x_flat = x.view(batch_size * num_slices, C, H, W)\n",
        "        features = self.encoder(x_flat)\n",
        "        features = features.view(batch_size, num_slices, self.feature_dim)\n",
        "\n",
        "        if self.use_slice_transformer:\n",
        "            # Adiciona positional encoding\n",
        "            features = features + self.slice_pos_embedding\n",
        "\n",
        "            # Transformer\n",
        "            features = self.slice_transformer(features)\n",
        "\n",
        "            # Attention pooling: aprende quais slices são mais importantes\n",
        "            attention_scores = self.attention_pool(features)  # (batch_size, num_slices, 1)\n",
        "            attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "\n",
        "            # Weighted sum dos slices\n",
        "            pooled = (features * attention_weights).sum(dim=1)  # (batch_size, feature_dim)\n",
        "\n",
        "            output = self.classifier(pooled)\n",
        "        else:\n",
        "            combined = features.view(batch_size, -1)\n",
        "            output = self.classifier(combined)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIxLCxeT6CeH"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs)\n",
        "    except:\n",
        "        auc = 0.0\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    # Calcula sensibilidade e especificidade\n",
        "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    metrics = {\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'sensitivity': sensitivity,\n",
        "        'specificity': specificity,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'confusion_matrix': cm.tolist()\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                num_epochs, device, scheduler=None, early_stopping_patience=5):\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_wts = None\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_acc'].append(val_metrics['accuracy'])\n",
        "\n",
        "        # Atualiza scheduler com base na perda de validação\n",
        "        if scheduler:\n",
        "            scheduler.step(val_metrics['loss'])\n",
        "\n",
        "        print(f\"Época {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_metrics['loss']:.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
        "\n",
        "        # Early stopping baseado na loss (melhor indicador de overfitting)\n",
        "        if val_metrics['loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['loss']\n",
        "            best_model_wts = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\"Early stopping na época {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    if best_model_wts:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chuWRvqKvWJy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "# Função para desnormalizar as imagens (se estiver usando normalização)\n",
        "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\"Desnormaliza um tensor de imagem\"\"\"\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "# Função para mostrar um batch de imagens\n",
        "def show_batch(dataloader, num_images=16, denorm=True):\n",
        "    \"\"\"\n",
        "    Mostra um batch de imagens do dataloader\n",
        "\n",
        "    Args:\n",
        "        dataloader: seu train_loader ou val_loader\n",
        "        num_images: quantidade de imagens para mostrar\n",
        "        denorm: se True, desnormaliza as imagens\n",
        "    \"\"\"\n",
        "    # Pega um batch\n",
        "    images, labels = next(iter(dataloader))\n",
        "\n",
        "    # Limita ao número solicitado\n",
        "    images = images[:num_images]\n",
        "    labels = labels[:num_images]\n",
        "\n",
        "    # Desnormaliza se necessário\n",
        "    if denorm:\n",
        "        images = denormalize(images.clone())\n",
        "\n",
        "    # Clipar valores para [0, 1]\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "\n",
        "    # Cria grid de imagens\n",
        "    grid = torchvision.utils.make_grid(images, nrow=4, padding=2)\n",
        "\n",
        "    # Converte para numpy\n",
        "    grid_np = grid.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Plota\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.imshow(grid_np)\n",
        "    plt.title(f'Batch de imagens - Labels: {labels.tolist()}')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Imprime informações\n",
        "    print(f\"Shape das imagens: {images.shape}\")\n",
        "    print(f\"Labels: {labels.tolist()}\")\n",
        "    print(f\"Min pixel value: {images.min().item():.3f}\")\n",
        "    print(f\"Max pixel value: {images.max().item():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfMKDEPC6GqK"
      },
      "outputs": [],
      "source": [
        "class ExperimentPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline para treinar modelo com diferentes quantidades de slices\n",
        "    \"\"\"\n",
        "    def __init__(self, metadata_df, base_path, results_dir, device):\n",
        "        self.metadata_df = metadata_df\n",
        "        self.base_path = base_path\n",
        "        self.results_dir = results_dir\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        self.device = device\n",
        "\n",
        "        # Transformações\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "\n",
        "        # Armazena resultados de todas as execuções\n",
        "        self.all_results = []\n",
        "\n",
        "    def run_experiment(self, slices_per_view, batch_size=2, num_epochs=20, lr=1e-5):\n",
        "        \"\"\"\n",
        "        Executa um experimento com N slices por orientação\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EXPERIMENTO: {slices_per_view} slices por orientação ({slices_per_view*3} total)\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # IMPORTANTE: Primeiro valida o dataset completo\n",
        "        print(\"Validando disponibilidade de slices...\")\n",
        "        valid_metadata = self._filter_valid_patients(self.metadata_df, slices_per_view)\n",
        "\n",
        "        if len(valid_metadata) == 0:\n",
        "            print(\"Nenhum paciente válido encontrado!\")\n",
        "            return None\n",
        "\n",
        "        print(f\"✓ {len(valid_metadata)} pacientes com slices suficientes\")\n",
        "        print(f\"   Distribuição: {valid_metadata['diagnosis'].value_counts().to_dict()}\\n\")\n",
        "\n",
        "        # DEPOIS faz o split\n",
        "        train_df, val_df = train_test_split(\n",
        "            valid_metadata,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=valid_metadata['diagnosis']\n",
        "        )\n",
        "\n",
        "        # Datasets (SEM validação interna, já foi feita)\n",
        "        train_dataset = FMRISkimmedDataset(\n",
        "            train_df, self.base_path, self.transform, slices_per_view,\n",
        "            validate=False  # Importante!\n",
        "        )\n",
        "        val_dataset = FMRISkimmedDataset(\n",
        "            val_df, self.base_path, self.transform, slices_per_view,\n",
        "            validate=False  # Importante!\n",
        "        )\n",
        "\n",
        "\n",
        "        from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "        # Conta quantas amostras há por classe\n",
        "        class_counts = train_df['diagnosis'].value_counts()\n",
        "        print(f\"Distribuição original: {class_counts.to_dict()}\")\n",
        "\n",
        "        # Calcula peso inverso (classe rara tem peso maior)\n",
        "        weights_per_class = {\n",
        "            label: 1.0 / count for label, count in class_counts.items()\n",
        "        }\n",
        "\n",
        "        # Gera vetor de pesos por amostra\n",
        "        sample_weights = train_df['diagnosis'].map(weights_per_class).values\n",
        "\n",
        "        # Cria sampler com oversampling proporcional\n",
        "        sampler = WeightedRandomSampler(\n",
        "            weights=torch.DoubleTensor(sample_weights),\n",
        "            num_samples=len(train_df),     # mantém o tamanho do dataset\n",
        "            replacement=True\n",
        "        )\n",
        "\n",
        "        # DataLoader com sampler\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=sampler,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Loader de validação sem oversampling\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        print(f\"Dataset: {len(train_dataset)} treino, {len(val_dataset)} validação\")\n",
        "\n",
        "        # Modelo\n",
        "        total_slices = slices_per_view * 3\n",
        "        model = MultiSliceViT(num_slices=total_slices, num_classes=2).to(self.device)\n",
        "\n",
        "        # Otimização\n",
        "        # Calcula pesos com base na proporção das classes\n",
        "        class_counts = valid_metadata['diagnosis'].value_counts().sort_index()\n",
        "        total = class_counts.sum()\n",
        "        weights = [total / class_counts[i] for i in range(len(class_counts))]\n",
        "        weights = torch.tensor(weights, dtype=torch.float).to(self.device)\n",
        "\n",
        "        print(total)\n",
        "        print(weights)\n",
        "\n",
        "        # Loss ponderada\n",
        "\n",
        "        #criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "        # Treinamento\n",
        "        print(\"\\nIniciando treinamento...\\n\")\n",
        "        model, history = train_model(\n",
        "            model, train_loader, val_loader, criterion, optimizer,\n",
        "            num_epochs, self.device, early_stopping_patience=5\n",
        "        )\n",
        "\n",
        "        # Avaliação final\n",
        "        final_metrics = evaluate(model, val_loader, criterion, self.device)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"RESULTADOS FINAIS - {slices_per_view} slices por orientação:\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Acurácia:       {final_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Precisão:       {final_metrics['precision']:.4f}\")\n",
        "        print(f\"Recall:         {final_metrics['recall']:.4f}\")\n",
        "        print(f\"Sensibilidade:  {final_metrics['sensitivity']:.4f}\")\n",
        "        print(f\"Especificidade: {final_metrics['specificity']:.4f}\")\n",
        "        print(f\"F1-Score:       {final_metrics['f1']:.4f}\")\n",
        "        print(f\"AUC-ROC:        {final_metrics['auc']:.4f}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        #self.print_confusion_details(final_metrics['confusion_matrix'])\n",
        "\n",
        "        # Salva resultados\n",
        "        results = {\n",
        "            'slices_per_view': slices_per_view,\n",
        "            'total_slices': total_slices,\n",
        "            'metrics': final_metrics,\n",
        "            'history': history,\n",
        "            'config': {\n",
        "                'batch_size': batch_size,\n",
        "                'num_epochs': num_epochs,\n",
        "                'learning_rate': lr,\n",
        "                'train_size': len(train_dataset),\n",
        "                'val_size': len(val_dataset)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.all_results.append(results)\n",
        "        self._save_experiment_results(results, slices_per_view)\n",
        "\n",
        "        # Salva modelo\n",
        "        model_path = os.path.join(self.results_dir, f'model_{slices_per_view}slices.pth')\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _filter_valid_patients(self, metadata_df, min_slices_per_view):\n",
        "        \"\"\"\n",
        "        Filtra pacientes que têm slices suficientes em todas as orientações\n",
        "        \"\"\"\n",
        "        valid_indices = []\n",
        "\n",
        "        for idx, row in metadata_df.iterrows():\n",
        "            participant_id = row['participant_id']\n",
        "            slices_dir = os.path.join(self.base_path, f\"sub-{participant_id}\")\n",
        "\n",
        "\n",
        "            # Verifica se o diretório existe\n",
        "            if not os.path.exists(slices_dir):\n",
        "                continue\n",
        "\n",
        "            # Verifica se tem slices suficientes em todas as orientações\n",
        "            has_enough_slices = True\n",
        "            for orientation in ['sagital', 'coronal', 'axial']:\n",
        "                pattern = os.path.join(slices_dir, f\"{orientation}/slice_{orientation}_*_idx*.png\")\n",
        "                files = glob.glob(pattern)\n",
        "                if len(files) < min_slices_per_view:\n",
        "                    has_enough_slices = False\n",
        "                    break\n",
        "\n",
        "            if has_enough_slices:\n",
        "                valid_indices.append(idx)\n",
        "\n",
        "        return metadata_df.iloc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "    def _save_experiment_results(self, results, slices_per_view):\n",
        "        \"\"\"\n",
        "        Salva resultados de um experimento específico\n",
        "        \"\"\"\n",
        "        exp_dir = os.path.join(self.results_dir, f'exp_{slices_per_view}slices')\n",
        "        os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "        # Salva JSON com métricas\n",
        "        json_path = os.path.join(exp_dir, 'metrics.json')\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'slices_per_view': results['slices_per_view'],\n",
        "                'total_slices': results['total_slices'],\n",
        "                'accuracy': results['metrics']['accuracy'],\n",
        "                'precision': results['metrics']['precision'],\n",
        "                'recall': results['metrics']['recall'],\n",
        "                'sensitivity': results['metrics']['sensitivity'],\n",
        "                'specificity': results['metrics']['specificity'],\n",
        "                'f1': results['metrics']['f1'],\n",
        "                'auc': results['metrics']['auc'],\n",
        "                'confusion_matrix': results['metrics']['confusion_matrix'],\n",
        "                'config': results['config']\n",
        "            }, f, indent=4)\n",
        "\n",
        "        # Plot de matriz de confusão\n",
        "        self.print_confusion_details(results['metrics']['confusion_matrix'])\n",
        "\n",
        "        # Plot de curvas de aprendizado\n",
        "        self._plot_learning_curves(results['history'], exp_dir)\n",
        "\n",
        "\n",
        "    def _plot_learning_curves(self, history, save_dir):\n",
        "        \"\"\"\n",
        "        Plota e salva curvas de aprendizado\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        # Loss\n",
        "        ax1.plot(history['train_loss'], label='Treino', marker='o')\n",
        "        ax1.plot(history['val_loss'], label='Validação', marker='s')\n",
        "        ax1.set_xlabel('Época')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Curva de Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Accuracy\n",
        "        ax2.plot(history['train_acc'], label='Treino', marker='o')\n",
        "        ax2.plot(history['val_acc'], label='Validação', marker='s')\n",
        "        ax2.set_xlabel('Época')\n",
        "        ax2.set_ylabel('Acurácia')\n",
        "        ax2.set_title('Curva de Acurácia')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'learning_curves.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def run_all_experiments(self, slice_counts=[2, 4, 6, 8, 10, 12, 14, 16],\n",
        "                          batch_size=8, num_epochs=20, lr=1e-4):\n",
        "        \"\"\"\n",
        "        Executa todos os experimentos sequencialmente\n",
        "        \"\"\"\n",
        "        print(f\"\\nINICIANDO PIPELINE DE EXPERIMENTOS\")\n",
        "        print(f\"Configurações variadas de slices: {slice_counts}\")\n",
        "        print(f\"Batch size: {batch_size}, Épocas: {num_epochs}, LR: {lr}\\n\")\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        for slices in slice_counts:\n",
        "            try:\n",
        "                self.run_experiment(slices, batch_size, num_epochs, lr)\n",
        "            except Exception as e:\n",
        "                print(f\"\\nERRO no experimento com {slices} slices: {e}\\n\")\n",
        "                continue\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        print(f\"\\nPIPELINE COMPLETO!\")\n",
        "        print(f\"Tempo total: {duration}\")\n",
        "\n",
        "        # Gera relatório comparativo\n",
        "        self.generate_comparative_report()\n",
        "\n",
        "    def generate_comparative_report(self):\n",
        "        \"\"\"\n",
        "        Gera relatório comparativo de todos os experimentos\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"RELATÓRIO COMPARATIVO DE TODOS OS EXPERIMENTOS\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Tabela comparativa\n",
        "        comparison_data = []\n",
        "        for result in self.all_results:\n",
        "            comparison_data.append({\n",
        "                'Slices/View': result['slices_per_view'],\n",
        "                'Total Slices': result['total_slices'],\n",
        "                'Accuracy': f\"{result['metrics']['accuracy']:.4f}\",\n",
        "                'Precision': f\"{result['metrics']['precision']:.4f}\",\n",
        "                'Recall': f\"{result['metrics']['recall']:.4f}\",\n",
        "                'F1-Score': f\"{result['metrics']['f1']:.4f}\",\n",
        "                'AUC': f\"{result['metrics']['auc']:.4f}\",\n",
        "                'Sensitivity': f\"{result['metrics']['sensitivity']:.4f}\",\n",
        "                'Specificity': f\"{result['metrics']['specificity']:.4f}\"\n",
        "            })\n",
        "\n",
        "        df_comparison = pd.DataFrame(comparison_data)\n",
        "        print(df_comparison.to_string(index=False))\n",
        "\n",
        "        # Salva CSV\n",
        "        csv_path = os.path.join(self.results_dir, 'comparative_results.csv')\n",
        "        df_comparison.to_csv(csv_path, index=False)\n",
        "        print(f\"\\n✓ Resultados salvos em: {csv_path}\")\n",
        "\n",
        "        # Plot comparativo\n",
        "        self._plot_comparative_metrics()\n",
        "\n",
        "        # Identifica melhor configuração\n",
        "        if self.all_results: # Check if all_results is not empty\n",
        "            best_result = max(self.all_results, key=lambda x: x['metrics']['accuracy'])\n",
        "            print(f\"\\nMELHOR CONFIGURAÇÃO:\")\n",
        "            print(f\"   Slices por orientação: {best_result['slices_per_view']}\")\n",
        "            print(f\"   Acurácia: {best_result['metrics']['accuracy']:.4f}\")\n",
        "            print(f\"   AUC: {best_result['metrics']['auc']:.4f}\\n\")\n",
        "        else:\n",
        "            print(\"\\nNão foi possível identificar a melhor configuração, pois nenhum experimento foi concluído com sucesso.\")\n",
        "\n",
        "\n",
        "    def print_confusion_details(self, cm):\n",
        "      \"\"\"\n",
        "      Plota e salva matriz de confusão detalhada com contagens e percentuais\n",
        "      \"\"\"\n",
        "      # Calcula totais\n",
        "      total = np.sum(cm)\n",
        "      total_control = np.sum(cm[0, :])  # Total de CONTROL (linha 0)\n",
        "      total_schz = np.sum(cm[1, :])     # Total de SCHZ (linha 1)\n",
        "\n",
        "      # Extrai valores da matriz\n",
        "      tn, fp = cm[0, 0], cm[0, 1]  # True Negative, False Positive\n",
        "      fn, tp = cm[1, 0], cm[1, 1]  # False Negative, True Positive\n",
        "\n",
        "      # Calcula percentuais por classe\n",
        "      tn_pct = (tn / total_control * 100) if total_control > 0 else 0\n",
        "      fp_pct = (fp / total_control * 100) if total_control > 0 else 0\n",
        "      fn_pct = (fn / total_schz * 100) if total_schz > 0 else 0\n",
        "      tp_pct = (tp / total_schz * 100) if total_schz > 0 else 0\n",
        "\n",
        "      # Cria figura com duas subplots\n",
        "      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "      # ==================== SUBPLOT 1: Valores Absolutos ====================\n",
        "      sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "                  xticklabels=['CONTROL', 'SCHZ'],\n",
        "                  yticklabels=['CONTROL', 'SCHZ'],\n",
        "                  cbar_kws={'label': 'Contagem'},\n",
        "                  annot_kws={'size': 16, 'weight': 'bold'})\n",
        "      ax1.set_title('Matriz de Confusão - Valores Absolutos', fontsize=14, weight='bold')\n",
        "      ax1.set_ylabel('Classe Real', fontsize=12)\n",
        "      ax1.set_xlabel('Classe Predita', fontsize=12)\n",
        "\n",
        "      # ==================== SUBPLOT 2: Percentuais ====================\n",
        "      # Cria matriz de percentuais\n",
        "      cm_pct = np.array([[tn_pct, fp_pct],\n",
        "                        [fn_pct, tp_pct]])\n",
        "\n",
        "      sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Greens', ax=ax2,\n",
        "                  xticklabels=['CONTROL', 'SCHZ'],\n",
        "                  yticklabels=['CONTROL', 'SCHZ'],\n",
        "                  cbar_kws={'label': '% da Classe Real'},\n",
        "                  annot_kws={'size': 16, 'weight': 'bold'})\n",
        "      ax2.set_title('Matriz de Confusão - Percentuais por Classe', fontsize=14, weight='bold')\n",
        "      ax2.set_ylabel('Classe Real', fontsize=12)\n",
        "      ax2.set_xlabel('Classe Predita', fontsize=12)\n",
        "\n",
        "      # ==================== Adiciona texto explicativo ====================\n",
        "      textstr = (\n",
        "          f'ANÁLISE DETALHADA\\n'\n",
        "          f'─────────────────────────────────\\n'\n",
        "          f'Total de amostras: {total}\\n\\n'\n",
        "          f'CONTROL (Real: {total_control} amostras):\\n'\n",
        "          f'  ✓ Acertos: {tn} ({tn_pct:.1f}%)\\n'\n",
        "          f'  ✗ Erros:   {fp} ({fp_pct:.1f}%) → preditos como SCHZ\\n\\n'\n",
        "          f'SCHZ (Real: {total_schz} amostras):\\n'\n",
        "          f'  ✓ Acertos: {tp} ({tp_pct:.1f}%)\\n'\n",
        "          f'  ✗ Erros:   {fn} ({fn_pct:.1f}%) → preditos como CONTROL\\n\\n'\n",
        "          f'─────────────────────────────────\\n'\n",
        "          f'Acurácia Geral: {(tn + tp) / total * 100:.2f}%'\n",
        "      )\n",
        "\n",
        "      # Adiciona caixa de texto abaixo dos gráficos\n",
        "      fig.text(0.5, -0.15, textstr,\n",
        "              ha='center', va='top',\n",
        "              fontsize=11, family='monospace',\n",
        "              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.close()\n",
        "\n",
        "\n",
        "    def _plot_comparative_metrics(self):\n",
        "        \"\"\"\n",
        "        Plota gráficos comparativos de métricas\n",
        "        \"\"\"\n",
        "        # Filtra resultados válidos (onde run_experiment não retornou None)\n",
        "        valid_results = [r for r in self.all_results if r is not None]\n",
        "\n",
        "        if not valid_results:\n",
        "            print(\"\\nNão há resultados válidos para gerar gráficos comparativos.\")\n",
        "            return\n",
        "\n",
        "        slice_counts = [r['slices_per_view'] for r in valid_results]\n",
        "        accuracies = [r['metrics']['accuracy'] for r in valid_results]\n",
        "        f1_scores = [r['metrics']['f1'] for r in valid_results]\n",
        "        aucs = [r['metrics']['auc'] for r in valid_results]\n",
        "        sensitivities = [r['metrics']['sensitivity'] for r in valid_results]\n",
        "        specificities = [r['metrics']['specificity'] for r in valid_results]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "        # Accuracy\n",
        "        axes[0, 0].plot(slice_counts, accuracies, marker='o', linewidth=2, markersize=8)\n",
        "        axes[0, 0].set_xlabel('Slices por Orientação')\n",
        "        axes[0, 0].set_ylabel('Acurácia')\n",
        "        axes[0, 0].set_title('Acurácia vs Número de Slices')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        axes[0, 0].set_xticks(slice_counts)\n",
        "\n",
        "        # F1-Score\n",
        "        axes[0, 1].plot(slice_counts, f1_scores, marker='s', linewidth=2, markersize=8, color='green')\n",
        "        axes[0, 1].set_xlabel('Slices por Orientação')\n",
        "        axes[0, 1].set_ylabel('F1-Score')\n",
        "        axes[0, 1].set_title('F1-Score vs Número de Slices')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        axes[0, 1].set_xticks(slice_counts)\n",
        "\n",
        "        # AUC\n",
        "        axes[1, 0].plot(slice_counts, aucs, marker='^', linewidth=2, markersize=8, color='red')\n",
        "        axes[1, 0].set_xlabel('Slices por Orientação')\n",
        "        axes[1, 0].set_ylabel('AUC-ROC')\n",
        "        axes[1, 0].set_title('AUC-ROC vs Número de Slices')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].set_xticks(slice_counts)\n",
        "\n",
        "        # Sensitivity & Specificity\n",
        "        axes[1, 1].plot(slice_counts, sensitivities, marker='o', linewidth=2, markersize=8, label='Sensibilidade')\n",
        "        axes[1, 1].plot(slice_counts, specificities, marker='s', linewidth=2, markersize=8, label='Especificidade')\n",
        "        axes[1, 1].set_xlabel('Slices por Orientação')\n",
        "        axes[1, 1].set_ylabel('Score')\n",
        "        axes[1, 1].set_title('Sensibilidade & Especificidade vs Slices')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        axes[1, 1].set_xticks(slice_counts)\n",
        "        axes[1, 1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.results_dir, 'comparative_metrics.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"✓ Gráficos comparativos salvos em: {self.results_dir}/comparative_metrics.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY8tLoTH6NrD",
        "outputId": "b73619be-9b5b-4e40-d339-dbdb612fb539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando device: cpu\n",
            "\n",
            "Carregando metadados...\n",
            "Filtrando apenas pacientes CONTROL e SCHZ...\n",
            "Total de participantes CONTROL/SCHZ: 180\n",
            "Distribuição: {'CONTROL': 130, 'SCHZ': 50}\n",
            "\n",
            "DEBUG: Verificando estrutura de diretórios...\n",
            "Base path: /content/drive/MyDrive/TCC/resultados_global/slices\n",
            "✓ Total de pastas sub-* encontradas: 176\n",
            "Primeiras 3 pastas: ['sub-10159', 'sub-10171', 'sub-10189']\n",
            "\n",
            "Analisando: sub-10159\n",
            "Caminho completo: /content/drive/MyDrive/TCC/resultados_global/slices/sub-10159\n",
            "\n",
            "Subpastas encontradas: ['sagital', 'coronal', 'axial']\n",
            "\n",
            "================================================================================\n",
            "📂 Orientação 'sagital':\n",
            "   Caminho: /content/drive/MyDrive/TCC/resultados_global/slices/sub-10159/sagital\n",
            "   Arquivos .png: 16\n",
            "   Arquivos .npy: 16\n",
            "   Primeiros 3 .png: ['slice_sagital_01_idx040.png', 'slice_sagital_02_idx041.png', 'slice_sagital_04_idx043.png']\n",
            "   Primeiros 3 .npy: ['slice_sagital_02_idx041.npy', 'slice_sagital_01_idx040.npy', 'slice_sagital_04_idx043.npy']\n",
            "   Padrão glob encontrou: 16 arquivos\n",
            "   Exemplo: slice_sagital_01_idx040.png\n",
            "\n",
            "📂 Orientação 'coronal':\n",
            "   Caminho: /content/drive/MyDrive/TCC/resultados_global/slices/sub-10159/coronal\n",
            "   Arquivos .png: 16\n",
            "   Arquivos .npy: 16\n",
            "   Primeiros 3 .png: ['slice_coronal_01_idx020.png', 'slice_coronal_02_idx019.png', 'slice_coronal_03_idx021.png']\n",
            "   Primeiros 3 .npy: ['slice_coronal_01_idx020.npy', 'slice_coronal_02_idx019.npy', 'slice_coronal_03_idx021.npy']\n",
            "   Padrão glob encontrou: 16 arquivos\n",
            "   Exemplo: slice_coronal_01_idx020.png\n",
            "\n",
            "📂 Orientação 'axial':\n",
            "   Caminho: /content/drive/MyDrive/TCC/resultados_global/slices/sub-10159/axial\n",
            "   Arquivos .png: 16\n",
            "   Arquivos .npy: 16\n",
            "   Primeiros 3 .png: ['slice_axial_01_idx022.png', 'slice_axial_02_idx023.png', 'slice_axial_03_idx024.png']\n",
            "   Primeiros 3 .npy: ['slice_axial_01_idx022.npy', 'slice_axial_02_idx023.npy', 'slice_axial_04_idx021.npy']\n",
            "   Padrão glob encontrou: 16 arquivos\n",
            "   Exemplo: slice_axial_01_idx022.png\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Pressione Ctrl+C para interromper ou aguarde para continuar...\n",
            "================================================================================\n",
            "\n",
            "\n",
            "INICIANDO PIPELINE DE EXPERIMENTOS\n",
            "Configurações variadas de slices: [4]\n",
            "Batch size: 2, Épocas: 20, LR: 1e-05\n",
            "\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENTO: 4 slices por orientação (12 total)\n",
            "================================================================================\n",
            "\n",
            "Validando disponibilidade de slices...\n",
            "✓ 176 pacientes com slices suficientes\n",
            "   Distribuição: {'CONTROL': 126, 'SCHZ': 50}\n",
            "\n",
            "Distribuição original: {'CONTROL': 100, 'SCHZ': 40}\n",
            "Dataset: 140 treino, 36 validação\n",
            "176\n",
            "tensor([1.3968, 3.5200])\n",
            "\n",
            "Iniciando treinamento...\n",
            "\n",
            "[[26  0]\n",
            " [10  0]]\n",
            "Época 1/20 | Train Loss: 0.7175, Val Loss: 0.6037, Train Acc: 0.5214, Val Acc: 0.7222\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Configurações\n",
        "    BASE_PATH = \"/content/drive/MyDrive/TCC/resultados_global/slices\"\n",
        "    RESULTS_DIR = \"/content/drive/MyDrive/TCC/experiment_results\"\n",
        "    PARTICIPANTS_TSV = \"/content/drive/MyDrive/TCC/dataset/ds000030-download/participants.tsv\"\n",
        "\n",
        "    # Device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Usando device: {device}\\n\")\n",
        "\n",
        "    # Carrega metadados\n",
        "    print(\"Carregando metadados...\")\n",
        "    metadata_df = pd.read_csv(PARTICIPANTS_TSV, sep='\\t')\n",
        "    metadata_df['participant_id'] = metadata_df['participant_id'].str.replace('sub-', '')\n",
        "\n",
        "    # FILTRA APENAS CONTROL (1XXXX) e SCHZ (5XXXX)\n",
        "    print(\"Filtrando apenas pacientes CONTROL e SCHZ...\")\n",
        "    metadata_df = metadata_df[metadata_df['diagnosis'].isin(['CONTROL', 'SCHZ'])]\n",
        "\n",
        "    # Opcional: Filtra apenas IDs que começam com 1 ou 5\n",
        "    metadata_df['first_digit'] = metadata_df['participant_id'].astype(str).str[0]\n",
        "    metadata_df = metadata_df[metadata_df['first_digit'].isin(['1', '5'])]\n",
        "    metadata_df = metadata_df.drop(columns=['first_digit'])\n",
        "\n",
        "    metadata_df['label'] = metadata_df['diagnosis'].map({'CONTROL': 0, 'SCHZ': 1})\n",
        "\n",
        "    print(f\"Total de participantes CONTROL/SCHZ: {len(metadata_df)}\")\n",
        "    print(f\"Distribuição: {metadata_df['diagnosis'].value_counts().to_dict()}\\n\")\n",
        "\n",
        "    # DEBUG: Verifica estrutura de diretórios\n",
        "    print(\"DEBUG: Verificando estrutura de diretórios...\")\n",
        "    print(f\"Base path: {BASE_PATH}\")\n",
        "\n",
        "    # Lista alguns sujeitos\n",
        "    if os.path.exists(BASE_PATH):\n",
        "        subdirs = [d for d in os.listdir(BASE_PATH) if os.path.isdir(os.path.join(BASE_PATH, d)) and d.startswith('sub-')]\n",
        "        print(f\"✓ Total de pastas sub-* encontradas: {len(subdirs)}\")\n",
        "        print(f\"Primeiras 3 pastas: {subdirs[:3]}\\n\")\n",
        "\n",
        "        # Pega um exemplo para análise\n",
        "        if len(subdirs) > 0:\n",
        "            example_sub = subdirs[0]\n",
        "            example_path = os.path.join(BASE_PATH, example_sub)\n",
        "\n",
        "            print(f\"Analisando: {example_sub}\")\n",
        "            print(f\"Caminho completo: {example_path}\\n\")\n",
        "\n",
        "            # Lista subpastas de orientação\n",
        "            subfolders = [d for d in os.listdir(example_path) if os.path.isdir(os.path.join(example_path, d))]\n",
        "            print(f\"Subpastas encontradas: {subfolders}\\n\")\n",
        "\n",
        "            # Verifica cada orientação\n",
        "            print(\"=\"*80)\n",
        "            for orientation in ['sagital', 'coronal', 'axial']:\n",
        "                orientation_dir = os.path.join(example_path, orientation)\n",
        "\n",
        "                if os.path.exists(orientation_dir):\n",
        "                    # Lista arquivos .png\n",
        "                    png_files = [f for f in os.listdir(orientation_dir) if f.endswith('.png')]\n",
        "                    npy_files = [f for f in os.listdir(orientation_dir) if f.endswith('.npy')]\n",
        "\n",
        "                    print(f\"📂 Orientação '{orientation}':\")\n",
        "                    print(f\"   Caminho: {orientation_dir}\")\n",
        "                    print(f\"   Arquivos .png: {len(png_files)}\")\n",
        "                    print(f\"   Arquivos .npy: {len(npy_files)}\")\n",
        "\n",
        "                    if len(png_files) > 0:\n",
        "                        print(f\"   Primeiros 3 .png: {png_files[:3]}\")\n",
        "                    if len(npy_files) > 0:\n",
        "                        print(f\"   Primeiros 3 .npy: {npy_files[:3]}\")\n",
        "\n",
        "                    # Testa padrão de busca\n",
        "                    pattern = os.path.join(orientation_dir, f\"slice_{orientation}_*_idx*.png\")\n",
        "                    matched_files = glob.glob(pattern)\n",
        "                    print(f\"   Padrão glob encontrou: {len(matched_files)} arquivos\")\n",
        "                    if len(matched_files) > 0:\n",
        "                        print(f\"   Exemplo: {os.path.basename(matched_files[0])}\")\n",
        "                else:\n",
        "                    print(f\"Pasta '{orientation}' não existe!\")\n",
        "\n",
        "                print()\n",
        "\n",
        "    else:\n",
        "        print(f\"Caminho não existe: {BASE_PATH}\")\n",
        "\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    print(\"Pressione Ctrl+C para interromper ou aguarde para continuar...\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Inicializa pipeline\n",
        "    pipeline = ExperimentPipeline(\n",
        "        metadata_df=metadata_df,\n",
        "        base_path=BASE_PATH,\n",
        "        results_dir=RESULTS_DIR,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Executa todos os experimentos\n",
        "    pipeline.run_all_experiments(\n",
        "        #slice_counts=[2, 4, 6, 8, 10, 12, 14, 16],  # De 2 em 2 até 16\n",
        "        slice_counts=[4],  # De 2 em 2 até 16\n",
        "        batch_size=2,\n",
        "        num_epochs=20,\n",
        "        lr=1e-5\n",
        "    )\n",
        "\n",
        "    print(\"TODOS OS EXPERIMENTOS CONCLUÍDOS!\")\n",
        "    print(f\"Resultados salvos em: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtPtdFWVzeZD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def generate_gradcam(model, input_tensor, target_class=None, device='cuda', slice_idx=0):\n",
        "    \"\"\"\n",
        "    Gera Grad-CAM para um slice específico de entrada em modelos baseados em ViT.\n",
        "\n",
        "    Args:\n",
        "        model: modelo MultiSliceViT treinado\n",
        "        input_tensor: tensor de shape (1, num_slices, 1, H, W)\n",
        "        target_class: classe alvo (int). Se None, usa a classe predita\n",
        "        device: cuda ou cpu\n",
        "        slice_idx: índice do slice a ser visualizado\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_tensor = input_tensor.to(device).requires_grad_(True)\n",
        "\n",
        "    # Seleciona o slice desejado\n",
        "    x = input_tensor[:, slice_idx, :, :, :]  # (1, 1, H, W)\n",
        "\n",
        "    # Passa pelo encoder ViT\n",
        "    vit = model.encoder\n",
        "    with torch.no_grad():\n",
        "        features = vit._process_input(x)\n",
        "        B, N, _ = features.shape\n",
        "        cls_token = vit.cls_token.expand(B, -1, -1)\n",
        "        features = torch.cat((cls_token, features), dim=1)\n",
        "        features = features + vit.encoder.pos_embed[:, :features.size(1), :]\n",
        "        features = vit.encoder.dropout(features)\n",
        "\n",
        "    # Ativa hook na última camada de atenção\n",
        "    attn_weights = []\n",
        "    def hook_attention(module, input, output):\n",
        "        attn_weights.append(output[1].detach())\n",
        "\n",
        "    handle = vit.encoder.layers[-1].attention.attn_drop.register_forward_hook(hook_attention)\n",
        "\n",
        "    # Forward normal para obter logits\n",
        "    output = model(input_tensor)\n",
        "    pred_class = output.argmax(dim=1).item() if target_class is None else target_class\n",
        "\n",
        "    # Gradiente da classe alvo\n",
        "    model.zero_grad()\n",
        "    class_score = output[0, pred_class]\n",
        "    class_score.backward(retain_graph=True)\n",
        "\n",
        "    # Obtém o mapa de atenção médio\n",
        "    attn_map = attn_weights[-1].mean(dim=1).squeeze().cpu().numpy()  # (num_heads, tokens, tokens) → (tokens, tokens)\n",
        "    attn_map = attn_map[0, 1:]  # ignora CLS token\n",
        "    side = int(np.sqrt(attn_map.shape[0]))\n",
        "    attn_map = attn_map.reshape(side, side)\n",
        "    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
        "\n",
        "    # Converte o slice original para imagem\n",
        "    img_np = input_tensor[0, slice_idx, 0].detach().cpu().numpy()\n",
        "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "    img_np = cv2.resize(img_np, (attn_map.shape[1], attn_map.shape[0]))\n",
        "\n",
        "    # Combina atenção com imagem original\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * attn_map), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = 0.5 * heatmap + np.repeat(img_np[..., np.newaxis], 3, axis=2)\n",
        "    cam = cam / cam.max()\n",
        "\n",
        "    # Plota resultado\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_np, cmap='gray')\n",
        "    plt.title(f'Slice {slice_idx} Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cam)\n",
        "    plt.title(f'Grad-CAM (Classe {pred_class})')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    handle.remove()\n",
        "    return cam"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZvDyPb76kCKR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}